# TIP 0001: Contiguity Argument for Memory Consistency

| TIP            | 0001                                          |
|:---------------|:----------------------------------------------|
| authors:       | Alan Szepieniec and Ferdinand Sauer           |
| title:         | Contiguity Argument for Memory Consistency    |
| status:        | draft                                         |
| created:       | 2022-08-15                                    |
| issue tracker: | https://github.com/TritonVM/triton-vm/pull/35 |
| pdf:           | [tip-0001.pdf](tip-0001.pdf)                  |

**Abstract.** In the current specification, the memory-like tables `RamTable`, `JumpStackTable`, and `OpStackTable` do not satisfy memory-consistency. Specifically, they are vulnerable to [Yuncong's attack](https://github.com/TritonVM/triton-vm/issues/12), which exploits the unverified and thus possibly-incorrect sorting in these tables. This TIP addresses one part of the issue by introducing a new table argument, the *contiguity argument*. It establishes the contiguity of the regions of fixed memory pointer.
This note is a companion to TIP-0003, which establishes that in memory-like tables, jumps in the clock cycle can only be directed forward, never backward.
Together, TIP-0001 and TIP-0003 fix the memory consistency issue.

## Introduction

The memory-like tables `RamTable`, `JumpStackTable`, and `OpStackTable` should be sorted by memory pointer first, and by clock cycle second. When this correct sorting is not enforced, it gives rise to attack undermining memory-consistency.

[Part V](https://aszepieniec.github.io/stark-brainfuck/attack) of the [BrainSTARK tutorial](https://aszepieniec.github.io/stark-brainfuck/) shows that memory-consistency follows if, in the memory table, every sublist of rows with the same memory pointer forms a contiguous region. The sorting rule is just one way to guarantee this contiguity. The sorting by clock cycle within each contiguous region is still necessary.

This TIP proposes a subprotocol for establishing the contiguity of all regions with a given memory pointer.
This *contiguity argument* is a collection of one base column, two extension columns, two polynomial commitments, two consistency constraints, two randomized transition constraints, and one randomized terminal constraint.
The base column and consistency constraints enable conditioning on a changed memory pointer.
The first extension column is a running product similar to that of a conditioned permutation argument. The first randomized transition constraint verifies the correct accumulation of factors for updating this column
The second extension column is the formal derivative of the second. The second randomized transition constraint verifies the correct application of the product rule of differentiation to update this column.
The two commited-to polynomials are Bézout coefficients.
The terminal constraint takes the weighted sum of the running product and the formal derivative, where the weights are the Bézout coefficients, and equates it to one. This equation asserts the Bézout relation. It can only be satisfied if the greatest common divisor of the running product and its formal derivative is one – implying that no change in the memory pointer resets it to a value used earlier.

## Contiguity Argument

The contiguity argument is only needed for the Ram Table because the memory pointer there (`ramp`) can take any value. In contrast, the two other memory-like tables, OpStackTable and JumpStackTable, are stacks. After sorting by the memory pointer, checking the contiguity of the access pattern is easy: across two consecutive rows, the memory pointer either remains unchanged or increments by one.

Since the contiguity argument may have applications elsewhere, it is presented here in a generic language.

Let `ramp` be the column whose contiguity we wish to establish. We add one base column and two extension columns: the difference inverse `di`, the running product `rp`, and the formal derivative `fd`. Additionally, the prover commits to two polynomials $a(X)$ and $b(X)$, but these polynomials do not necessarily correspond to columns in the table.

The values contained in the extension columns are undetermined until the verifier's challenge $\alpha$ is known; before that happens it is worthwhile to present the polynomial expressions in $X$, anticipating the substitution $X \mapsto \alpha$.

The difference inverse `di` takes the inverse of the difference between the current and next `ramp` values if that difference is non-zero, and zero else. This constraint corresponds to two consistency constraint polynomials:

 - $(\mathsf{ramp}^\star - \mathsf{ramp})\cdot((\mathsf{ramp}^\star - \mathsf{ramp}) \cdot \mathsf{di} - 1)$, and
 - $\mathsf{di}\cdot((\mathsf{ramp}^\star - \mathsf{ramp}) \cdot \mathsf{di} - 1)$.

The running product `rp` starts with $1$ initially and accumulates a factor $X - \mathsf{ramp}$ in every pair of rows where $\mathsf{ramp} \neq \mathsf{ramp}^\star$. This evolution corresponds to one transition constraint: $(\mathsf{ramp}^\star - \mathsf{ramp}) \cdot (\mathsf{rp}^\star - \mathsf{rp} \cdot (X - \mathsf{ramp})) + (1 -(\mathsf{ramp}^\star -\mathsf{ramp}) \cdot \mathsf{di}) \cdot (\mathsf{rp}^\star - \mathsf{rp})$.

Denote by $f_{\mathsf{rp}}(X)$ the polynomial that accumulates all factors $X - \mathsf{ramp}$ in every pair of rows where $\mathsf{ramp} \neq \mathsf{ramp}^\star$. If the last row is a padding row then the value in the $\mathsf{rp}$ column is identical to that of $f_{\mathsf{rp}}(X)$. If the last row is not a padding row then $\mathsf{rp}$ has not accumulated the factor $X - \mathsf{ramp}$ yet, and so the relevant relation is $f_{\mathsf{rp}}(X) = \mathsf{rp} \cdot ( X - \mathsf{ramp} )$.

The column `fd` contains the “formal derivative” of the running product with respect to $X$. The formal derivative is initially $0$. The transition constraint applies the product rule of differentiation conditioned upon the difference in `ramp` being nonzero; in other words, if $\mathsf{ramp} = \mathsf{ramp}^\star$ then the same value persists; but if $\mathsf{ramp} \neq \mathsf{ramp}^\star$ then $\mathsf{fd}$ is mapped as

$$ \mathsf{fd} \mapsto \mathsf{fd}^ \star = (X - \mathsf{ramp}) \cdot \mathsf{fd} + \mathsf{rp}   . $$

This update rule is called the *product rule of differentiation* because, assuming $\mathsf{ramp}^\star \neq \mathsf{ramp}$, then

$$\begin{aligned}
\frac{\mathsf{d}  \mathsf{rp}^\star}{\mathsf{d}   X} &= \frac{\mathsf{d}  (X - \mathsf{ramp}) \cdot \mathsf{rp}}{\mathsf{d}   X} \\
&= (X - \mathsf{ramp}) \cdot \frac{\mathsf{d}   \mathsf{rp}}{\mathsf{d}   X} + \frac{\mathsf{d}  ( X - \mathsf{ramp})}{\mathsf{d}   X} \cdot \mathsf{rp} \\
&= (X - \mathsf{ramp}) \cdot \mathsf{fd} +\mathsf{rp} \enspace .
\end{aligned}$$

The transition constraint for $\mathsf{fd}$ is $(\mathsf{ramp}^\star - \mathsf{ramp}) \cdot (\mathsf{fd}^\star - \mathsf{rp} - (X - \mathsf{ramp}) \cdot \mathsf{fd}) + (1 -(\mathsf{ramp}^\star -\mathsf{ramp}) \cdot \mathsf{di}) \cdot (\mathsf{fd}^\star - \mathsf{fd})$.

Let $f_{\mathsf{fd}}(X)$ denote the polynomial derived from $f_{\mathsf{rp}}(X)$ with all steps of the product rule applied. If the last row is a padding row, then $f_{\mathsf{fd}}(X)$ is identically the value $\mathsf{fd}$. However, if the last row is not a padding row then the product rule must be applied one last time. In this case the relevant relation is $f_{\mathsf{fd}} = \mathsf{rp} + \mathsf{fd} \cdot (X - \mathsf{ramp})$.

The polynomials $a(X)$ and $b(X)$ are *randomized* Bézout coefficients of the relation

$$ a(X) \cdot f_{\mathsf{rp}}(X) + b(X) \cdot f_{\mathsf{fd}}(X) = \gcd(f_{\mathsf{rp}}(X), f_{\mathsf{fd}}(X)) \enspace .$$

The prover finds $a(X)$ and $b(X)$ as $a(X) = a^*(X) + k \cdot f_{\mathsf{fd}}(X)$ and $b(X) = b^*(X) - k \cdot f_{\mathsf{rp}}(X)$, where $a^*(X)$ and $b^*(X)$ are the minimal-degree Bézout coefficients as returned by the extended Euclidean algorithm, and where $k \in \mathbb{F}$ is a uniformly random field element called the *Bézout randomizer*. For the sake of the soundness analysis, set the degree bound on $a(X)$ and $b(X)$ to $T$, the height of the table.

When the verifier supplies $\alpha$, the prover responds with (among other things) the randomized Bézout coefficients $A = a(\alpha)$ and $B = b(\alpha)$. The verifier verifies their correct calculation using the [DEEP technique](https://eprint.iacr.org/2019/336): in addition to adding $a(X)$ and $b(X)$, the prover adds $q_{a}(X) = \frac{a(X) - A}{X - \alpha}$ and $q_{b}(X) = \frac{b(X) - B}{X - \alpha}$ to the nonlinear combination, and the verifier verifies that it was added. The verifier additionally verifies the randomized AIR constraint $A \cdot f_{\mathsf{rp}}(\alpha) + B \cdot f_{\mathsf{fd}}(\alpha) = 1$. Recall that if the last row is a padding row then $f_{\mathsf{rp}}(X) = \mathsf{rp}$ and $f_{\mathsf{fd}}(X) = \mathsf{fd}$; whereas if it is not a padding row then $f_{\mathsf{rp}}(X) = \mathsf{rp} \cdot ( X - \mathsf{ramp} )$ and $f_{\mathsf{fd}} = \mathsf{rp} + \mathsf{fd} \cdot (X - \mathsf{ramp})$.

**Completeness.** The prover is incapable of proving that the gcd equals 1 when $f_{\mathsf{rp}}(X)$ and $f_{\mathsf{fd}}(X)$ share a factor. Therefore, the gcd can only be non-one for dishonest provers. As a result, the protocol has perfect completeness. $\square$

**Soundness.** If the table has at least one non-contiguous region, then $f_{\mathsf{rp}}(X)$ and $f_{\mathsf{fd}}(X)$ share at least one factor. As a result, no Bézout coefficients $a(X)$ and $b(X)$ can exist such that $a(X) \cdot f_{\mathsf{rp}}(X) + b(X) \cdot f_{\mathsf{fd}}(X) = 1$. The verifier therefore probes unequal polynomials of degree at most $2T$. According to the Schwartz-Zippel lemma, the false positive probability is at most $2T / \vert \mathbb{F} \vert$. $\square$

**Zero-Knowledge.** The standard workflow of interpolating columns with randomizers guarantees that the columns do not leak information. However, the same argument does not apply for the polynomial commitments to $a(X)$ and $b(X)$. The transcripts contains the evaluations of these polynomials in $4t+1$ points, where $t$ is the number of colinearity checks in FRI and where the $+1$ comes from the evaluation $a(\alpha)$ and $b(\alpha)$.

Let $D$ be the domain of $4t+1$ points where $a(X)$ and $b(X)$ are evaluated. Let $E$ be the list of $8t+2$ evaluations of $a(X)$ and $b(X)$ on $D$. There is an affine relation between $E$ and the $8t+2$ uniformly random coefficients of $k(X)$.

$$ \left( \begin{matrix}
\vdots \\
a(x) \\
\vdots \\ \hline
\vdots \\
b(x) \\
\vdots
\end{matrix} \right) = \left( \begin{matrix}
\vdots \\
a^\star(x) \\
\vdots \\ \hline
\vdots \\
b^\star(x) \\
\vdots
\end{matrix} \right) + \left( \begin{array}{ccc|ccc}
\ddots & & & & \\
& f_{\mathsf{fd}}(x) & & & \\
& & \ddots  & & \\ \hline
& & & \ddots & & \\
& & & & -f_{\mathsf{rp}}(x) & \\
& & & & & \ddots \\
\end{array} \right) \left( \begin{matrix}
\vdots & & \vdots \\
x^0 & \cdots & x^{8t+1} \\
\vdots & & \vdots
\end{matrix} \right) \left( \begin{matrix}
k_0 \\
k_1 \\
\vdots \\
k_{8t+1}
\end{matrix} \right) $$

Note that $a(X)$ and $b(X)$ cannot vary independently because they are constrained by the Bézout relation

$$ a(X) f_{\mathsf{rp}}(X) + b(X) f_{\mathsf{fd}}(X) = 1 \enspace .$$

Let $x$ be one of the points where the evaluations of $a(X)$ and $b(X)$ are transmitted as part of the protocol. Distinguish four cases.

 1. $f_{\mathsf{rp}}(x) = f_{\mathsf{fd}}(x) = 0$. In this case the prover fails to convince the verifier.
 2. $0 = f_{\mathsf{rp}}(x) \neq f_{\mathsf{fd}}(x)$. In this case $b(x)$ does not leak information because it is constrained by the Bézout relation.
 3. $f_{\mathsf{rp}}(x) \neq f_{\mathsf{fd}}(x) = 0$. In this case $a(x)$ does not leak information because it is constrained by the Bézout relation.
 4. $f_{\mathsf{rp}}(x) \cdot f_{\mathsf{fd}}(x) \neq 0$. In this case it suffices to show that $a(x)$ is sufficiently random because this fixes $b(x)$ to $\frac{1 - a(x)f_{\mathsf{rp}}(x)}{f_{\mathsf{fd}}(x)}$.


Given $f_{\mathsf{rp}}(X)$ and $f_{\mathsf{fd}}(X)$, a value for $a(X)$ fixes the matching value for $b(X)$. $\leftarrow$ That's actually not true!

Therefore, it suffices to show that for every point $x$ in which both $a(X)$ and $b(X)$ are evaluated, either $a(x)$ or $b(x)$ is sufficiently random.

Let $P(x) : \mathbb{F} \rightarrow \lbrace 0, 1 \rbrace$ be any predicate. Let $D$ be the domain of $4t+1$ points where $a(X)$ and $b(X)$ are evaluated. It suffices to show that the list of selected evaluations $V_P = \lbrace P(x) \cdot a(x) + (1-P(x)) \cdot b(x) \quad \vert \quad x \in D \rbrace$ is independently uniformly distributed for *some* predicate $P$.

Recall that $a(X) = a^\star(X) + k(X) \cdot f_{\mathsf{fd}}(X)$ and $b(X) = b^\star(X) - k(X) \cdot f_{\mathsf{rp}}(X)$. It is possible to find an affine relation between the $4t+1$ observed values of $V_P$ and the $4t+1$ uniformly random coefficients of $k(X) = \sum_{i=0}^{4t} k_iX^i$. Specifically:

$$ P(x) \cdot a(x) + (1 - P(x)) \cdot b(x) $$

$$ = P(x) \cdot \left( a^\star(x) + f_{\mathsf{fd}}(x) \cdot \left( \begin{matrix}
x^0 & x^1 & x^2 & \cdots & x^{4t}
\end{matrix} \right) \left( \begin{matrix}
k_0 \\
k_1 \\
k_2 \\
\vdots \\
k_{4t}
\end{matrix} \right) \right) $$

$$ + (1 - P(x)) \cdot \left( b^\star(x) - f_{\mathsf{rp}}(x) \cdot \left( \begin{matrix}
x^0 & x^1 & x^2 & \cdots & x^{4t}
\end{matrix} \right) \left( \begin{matrix}
k_0 \\
k_1 \\
k_2 \\
\vdots \\
k_{4t}
\end{matrix} \right) \right) $$

$$ = P(x) \cdot a^\star(x) + (1 - P(x)) \cdot b^\star(x) + \left( P(x) \cdot f_{\mathsf{fd}}(x) - (1 - P(x)) \cdot f_{\mathsf{rp}}(x) \right) \cdot \left( \begin{matrix}
x^0 & x^1 & x^2 & \cdots & x^{4t}
\end{matrix} \right) \left( \begin{matrix}
k_0 \\
k_1 \\
k_2 \\
\vdots \\
k_{4t}
\end{matrix} \right) $$

The coefficient matrix of this affine relation is square and Vandermonde up to left-multiplication by a diagonal matrix. Assume this diagonal matrix is invertible, then the coefficient matrix of this affine relation is invertible also. As a result, it is possible to find coefficients $k_0, \ldots, k_{4t}$ that explain any view of $4t+1$ observed evaluations $V$.

To see why the diagonal matrix is invertible, observe that the entries are $P(x) \cdot f_{\mathsf{fd}}(x) - (1 - P(x)) \cdot f_{\mathsf{rp}}(x)$. Choose the predicate 

$$ P(x) = \left \lbrace \begin{matrix} 1 & \Leftarrow & \exists i . x = \mathit{ramp}_i \\
0 & \Leftarrow & \forall i . x \neq \mathit{ramp}_i \end{matrix} \right. $$

and recall that 

$$ f_{\mathsf{fd}}(X) = \sum_{i} \prod_{j \neq i} (X - \mathit{ramp}_j) $$

$$ f_{\mathsf{rp}}(X) = \prod_i (X - \mathit{ramp}_i) $$

For any evaluation point $x$, either 

 1. $x = \mathit{ramp}_i$ for some $i$, in which case $P(x) = 1$ and $f_{\mathsf{fd}}(X)$ has exactly one term that does not contain a zero-factor, so $P(x) \cdot f_{\mathsf{fd}}(x) - (1 - P(x)) \cdot f_{\mathsf{rp}}(x) = f_{\mathsf{fd}}(x) \neq 0$;
 2. $x \neq \mathit{ramp}_i$ for all $i$, in which case $P(x) = 0$ and all factors of $f_{\mathsf{rp}}(X)$ are nonzero, so $P(x) \cdot f_{\mathsf{fd}}(x) - (1 - P(x)) \cdot f_{\mathsf{rp}}(x) = -f_{\mathsf{rp}}(x) \neq 0$. $\square$